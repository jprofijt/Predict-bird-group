---
title: "Predicting bird classifcation by bone lenghts"
author: "Jouke Profijt"
date: "October 8, 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
license: GPLv3
---
```{r echo=FALSE}
#Copyright (c) 2018 Jouke Profijt.
#Licensed under GPLv3. See LICENSE
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
# Classification of differing ecological bird populations

## Introduction

## Materials & Methods

During this research we used data from an external source, used weka 3.8.0 for classification and R version 3.5.1 in combination with Rstudio for data analysis.

### Data

[Birds' Bones and Living Habits, Kaggle dataset](https://www.kaggle.com/zhangjuefei/birds-bones-and-living-habits)
    
  Bone measurements were measured from a skeleton collection of
  Natural History Museum of Los Angeles County, 
  provided by Dr. D. Liu of beijing Museaum of Natural History

  
### R and Rstudio
R was used for the exploration of the data, cleaning of the data and statistical analysis. 
In our EDA(Exploratory Data Analysis) we looked at the structure of the data and the distribution of hte data using basic graphs with the ggplot2 library. 

Cleaning was done based on missing values which we all removed & on outliers which we also removed,

And lastly we compared different classification algorithms again using plots from ggplot2 to choose the correct algorithm.
  
### weka 3.8.0

Weka (Waikato Environment for Knowledge Analysis) is a free to use datamining software written in Java. It is an java application that is capable of doing lots of things apliccable to datamining. We will be using it to determine what is the best classification algorithm for our usecase. As we will be creating a java application of our own that can be used to classify the different bird groups. 

* First we use the explorer to make our datafiles usable in weka by removing duplicate id collums
* when the data is ready we can use the classify module in weka to test diffrent classification algorithm's that have reasonable accuracy's.
* at last when we found some interesting algorithm's we can use the experimenter to try diffrent settings to find optimal ones.

Every algorithm their initial output is a confusion matrix. This matrix is the measurement that we use to compare different algorithms. they are also inspected using a learning & ROC curve's.

* A Learning curve shows the false positive rate over how much data is provided to the learning algorithm
* A ROC curve shows the Sensitivity over the Specificity.



    
  

## Resutls


### EDA
```{r echo=FALSE}
BirdBones <- read.csv("../data/bird.csv",header = T, sep = ",")
BirdBones.noNA <- BirdBones[complete.cases(BirdBones),]
length <- c(2,4,6,8,10)
# library(ggplot2)
# 
# ggplot(stack(BirdBones.noNA[length]), aes(x = ind, y = values, color = ind)) +
#   geom_boxplot()+
#   xlab("Bone Type")+
#   ylab("Length in mm")+
#   labs(title = "Boxplot for bone lenghts per bone in mm", 
#        subtitle = "Variation between small and large bones.",
#        caption = "Graph 1: Bone lengths")
```
<!-- Graph 1 shows the average lenght per bone for all ecological groups. As we can see there are 2 bone types which are quite a bit smaller than the other 3 bones. The Femuer(green) is the smallest of these 2, this stands out because in us humans this is our biggest bone. after the Femeur the next smallest bone is the Taesometatarsus, this is expected as this bone connects the feet of the birds to their legs. -->

<!-- These 2 bones are quite small and show less variation than the other 3 bones. As we are trying to classify these bones they might be less important in our final classification algorithm.  -->

```{r, echo=FALSE}
library(ggplot2)
library(grid)
library(gridExtra)
source("../scripts/BoneMeans.R")
length.long <- c(2, 4, 8)
diameter.long <- c(3, 5, 9)
BirdBones.noNA.long <- BoneMeans(BirdBones.noNA, length.long, diameter.long)

p1<- ggplot(BirdBones.noNA.long,aes(x=length.mean,y=diameter.mean,color=type))+
  geom_point()+
  geom_smooth(method = loess)+
  xlab("Mean Lenght in mm")+
  ylab("Mean Diameter in mm")+
  labs(title = "lenght vs mean diameter", caption = "graph 1: lenght vs diameter"
       )+
  theme(legend.position="bottom")

p2 <- ggplot(BirdBones.noNA.long,aes(x=log2(length.mean),y=log2(diameter.mean),color=type))+
  geom_point()+
  geom_smooth(method = loess)+
  xlab("Log2 Mean Lenght")+
  ylab("Log2 Mean Diameter")+
  labs(title = "Log2 transformed", caption = "graph 2: Log2 lenght vs diameter"
       )

g_legend<-function(a.gplot){
  tmp <- ggplot_gtable(ggplot_build(a.gplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)}

mylegend <-g_legend(p1)


p3 <- grid.arrange(arrangeGrob(p1 + theme(legend.position="none"),
                         p2 + theme(legend.position="none"),
                         nrow=1),
             mylegend, nrow=2,heights=c(10, 1))


```

Graph 1 & 2 show the bone length distribution. Graph 1 is the distribution without any changes. Here we see that the Swimming birds have quite large bones, which could be because of abnormally big bird samples. as you look closer at the swimming birds in graph 1 and 2 we see that there might be a lot of noise in these samples because there are a lot of points outside the main distribution. For the other groups we also see that the Scansorial Birds & the Sininging birds are the smaller groups. 

Terrestrial Birds, Raptors and Wading Birds seem to be in the middle of the rest with the Terrestrial Birds having notisable thicker bones that the others.

## Data Cleaning
```{r, echo=FALSE}
library(ggplot2)
library(grid)
library(gridExtra)

 p1 <- ggplot(BirdBones.noNA.long, aes(x = type, y = length.mean, color = type)) +
  geom_boxplot()+
  ggtitle("Boxplot for each ecological group's mean bone lenght")+
  ylab("Mean bone lenght in mm")+
  xlab("ecological group")+
  labs("Boxplot for each ecological group's mean bone lenght", subtitle = "For Humerus, Ulna and Tibiotarsus",caption = "graph 3: mean bone lenghts")+
   theme(legend.position="none")





huml.3rd.q <- 90.31
huml.1st.q <-25.17

out <- huml.1st.q - 1.5*(huml.3rd.q - huml.1st.q)
out.large <- huml.3rd.q + 1.5*(huml.3rd.q - huml.1st.q)
outliers <- subset(BirdBones.noNA.long, huml > out.large | huml < out)
Birdbones.Clean <- BirdBones.noNA.long[! BirdBones.noNA.long$id %in% outliers$id, ]



p2 <- ggplot(Birdbones.Clean, aes(x = type, y = length.mean, color = type)) +
  geom_boxplot()+
  labs(
    title = "Boxplot for each ecological group's mean bone lenght",
    x = "ecological group",
    y = "",
    subtitle = "For Humerus, Ulna and Tibiotarsus",
    caption = "graph 4: Cleaned data"
  )+
  theme(legend.position="none")



grid.arrange(p1, p2, ncol = 2)
```
For our cleaning of data we removed all rows with NA's which cost us 7 datapoints. After that we calculated the 1st and 3rd quartile outliers from the humurus which can be seen in graph 3 & 4, and removed 9 rows. 

After all cleaning we left with 404 datapoints to use for Weka Analyses and classification.


## Weka
In weka the goal was to find a classification algorithem that has the highest possible accuracy because classifying a birds herritage it does not matter that much if there is a False positive, the goal was to keep false negatives low and keep true positives high.

```{r}
rocdataW <- read.csv("../data/RocWading.arff", header = F, comment.char = "@", na.strings = "?")
rocdataWibk <- read.csv("../data/RocWadingIbk.arff", header = F, comment.char = "@", na.strings = "?")

library(ggplot2)
ggplot()+
  geom_line(aes(x = rocdataW$V6, y = rocdataW$V7, color="Random.Forest"))+
  geom_line(aes(x = rocdataWibk$V6, y = rocdataWibk$V7, color="IBk"))+
  labs(x = "Specificity",
       y = "Sensitivity",
       title = "ROC curve",
       subtitle = "Based on algorithm preformance on Wading bird classification",
       caption = "Graph 6: ROC"
       )
```

```{r}
RemovedPercentage <- c(10, 20, 30, 40, 50, 60, 70, 80, 90)
Random.Forest <- c(22.07, 23.27, 25.17, 26.82, 29.17, 32.59, 35.48, 39.24, 47.05)
IBk <- c(18.26, 19.08, 20.63, 23.01, 25.26, 28.52, 31.58, 35.75, 45.99)

dataf <- data.frame(rev(RemovedPercentage), Random.Forest, IBk)
dataf <- dataf[seq(dim(dataf)[1],1),]

library(ggplot2)
ggplot(dataf)+
  geom_smooth(aes(x = RemovedPercentage, y = Random.Forest, color="Random.Forest"))+
  geom_smooth(aes(x = RemovedPercentage, y = IBk, color="IBk"))+
  labs(x = "Percentage of data",
       y = "False Positive rate",
       title = "Learning curve",
       subtitle = "For Random.Forest && IBk",
       caption = "graph 7: Learning curve inspected algorithm's"
       )
  
```


```{r, include=FALSE}
library(pander)
 conf.matrix <-
   rbind(
    c(0,0,0,0,0,108),
    c(0,0,0,0,0,63),
    c(0,0,0,0,0,23),
    c(0,0,0,0,0,48),
    c(0,0,0,0,0,38),
    c(0,0,0,0,0,124))
colnames(conf.matrix) <- c("SW", "W", "T", "R", "P", "SO")

row.names(conf.matrix) <- c("SW", "W", "T", "R", "P", "SO")

pander(conf.matrix, caption = "Zero R Confusion matrix")

```

```{r, include=FALSE}
accuracy <- rbind(c(124, 280), c("30.6931 %", "69.3069 %"))

colnames(accuracy) <- c("Correct", "Incorrect")
rownames(accuracy) <- c("Instances", "Percentage")
pander(accuracy, caption = "Zero R classification results")
```


```{r, include=FALSE}
 conf.matrix <-
   rbind(
    c(69,11,1,10,8,9),
    c(35,2,4,4,10,8),
    c(4,1,4,2,8,4),
    c(29,5,1,6,6,1),
    c(5,0,7,0,16,10),
    c(2,0,3,0,10,109))
colnames(conf.matrix) <- c("SW", "W", "T", "R", "P", "SO")

row.names(conf.matrix) <- c("SW", "W", "T", "R", "P", "SO")

pander(conf.matrix, caption = "One R Confusion matrix")
```
```{r, include=FALSE}
accuracy <- rbind(c(208, 196), c("51.4851 %", "48.5149 %"))

colnames(accuracy) <- c("Correct", "Incorrect")
rownames(accuracy) <- c("Instances", "Percentage")
pander(accuracy, caption = "One R classification results")
```


### Chosen classifier: Random.Forest

```{r, echo=FALSE}
 conf.matrix <-
   rbind(
    c(87,11,0,4,0,6),
    c(17,34,0,2,4,6),
    c(2,0,12,2,4,3),
    c(7,1,0,37,3,0),
    c(0,0,1,1,30,6),
    c(0,0,1,1,1,121))
colnames(conf.matrix) <- c("SW", "W", "T", "R", "P", "SO")

row.names(conf.matrix) <- c("SW", "W", "T", "R", "P", "SO")

pander(conf.matrix, caption = "Random.Forest Confusion matrix as chosen classifier")
```


```{r, echo=FALSE}
accuracy <- rbind(c(321, 83), c("79.4554 %", "20.5446 %"))

colnames(accuracy) <- c("Correct", "Incorrect")
rownames(accuracy) <- c("Instances", "Percentage")
pander(accuracy, caption = "Random.Forest as chosen classifier")
```


```{r}
humurus <- signif(84.6535 - 82.1782, 4)
ulna <-  signif(84.6535 -83.1683, 4) 
Femur <-  signif(84.6535 -80.4455, 4)
Tibiotarsus <-  signif(84.6535 - 84.4059, 4) 
Taesometatarsus <-  signif(84.6535-83.1683, 4)

loss <- data.frame(Bone=c("humurus", "ulna", "Femur", "Tibiotarsus", "Taesometatarsus"), 
                   Classify_loss=c(humurus, ulna, Femur, Tibiotarsus, Taesometatarsus))

library(ggplot2)
ggplot(data=loss, aes(x=Bone, y=Classify_loss)) +
  geom_bar(stat="identity", fill = "blue")+
  geom_text(aes(label=Classify_loss), vjust=1.6, color="white", size=3.5)+
  theme_minimal()+
  labs(x = "bone type",
       y = "accuracy loss (%)",
       title = "Classification loss if certain bones are removed",
       subtitle = "For Random.forest",
       caption = "graph 5: accuracy loss per bone"
       )
```
in graph 5 we can see what the importance is from certain bones if we are talking about classification. the diffrences we can see are becouse of their importance for the functionallity of the bird groups.





## Conclusion & discussion

What is the most important bone for each ecological group their function? we can conclude that (At least for classification) the Femur is the most important as when we remove this from our classification algorithems the accuracy loss is great. we can see this in graph 5 where the loss per bone is displayed. 
while researching the subject we made the assumption that the Femeur and Taesometatarsus were the least important for classification. Becouse of this the first classification algorithems were done only using the longer bones but further in reverted this desicion. The classification algorithem that was chosen however wasn't changed. In the future we should first collect some data about the importance for some bones before making such rational desisions.

We do wan to think about removing some bones as when we have less data to be put in the more unknown fossils with missing bones could possibly be classified

## Minor Proposal

## References
