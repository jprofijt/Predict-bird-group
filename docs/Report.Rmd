---
title: "Predicting bird classifcation by bone lenghts"
author: "Jouke Profijt"
date: "October 8, 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
license: GPLv3
---
```{r echo=FALSE}
#Copyright (c) 2018 Jouke Profijt.
#Licensed under GPLv3. See LICENSE
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
# Classification of differing ecological bird populations

## Introduction

Are there bones in a bird that are more important for classification using machine learning? Or should we take all avalable information into account if we are creating a classificiaton algorithm using machine learning. And then what algorithm should we use? 

In this research project we will be going through data of varying ecological bird groups. in this data the bone lengths and diameters are provided of an bird with a label to which ecological group they belong. 

Using this data we will be preparing this data for classification, analysing results of different classification algorithms and see how to use the data for classification.

We will be looking into Random.Forest and IBk asl classification algorithms. Random.Forest is tree / decision based and IBk is a K-nearest neigbour classifier(lazy). As IBk is lazy we have to be cautious becouse there might form a bias towards groups with bigger data values.

After we have decided what classfication algorithm to use we can use this research to create a java wrapper application to make use of this classifier in the future if we want to study an unknown fossil ans see what the possible ecological goup is.





\newpage

## Materials & Methods

During this research we used data from an external source, used weka 3.8.0 for classification and R version 3.5.1 in combination with Rstudio for data analysis.

### Data

[Birds' Bones and Living Habits, Kaggle dataset](https://www.kaggle.com/zhangjuefei/birds-bones-and-living-habits)
    
  Bone measurements were measured from a skeleton collection of
  Natural History Museum of Los Angeles County, 
  provided by Dr. D. Liu of beijing Museaum of Natural History

  
### R and Rstudio
R was used for the exploration of the data, cleaning of the data and statistical analysis. 
In our EDA(Exploratory Data Analysis) we looked at the structure of the data and the distribution of hte data using basic graphs with the ggplot2 library. 

Cleaning was done based on missing values which we all removed & on outliers which we also removed,

And lastly we compared different classification algorithms again using plots from ggplot2 to choose the correct algorithm.
  
### weka 3.8.0

Weka (Waikato Environment for Knowledge Analysis) is a free to use datamining software written in Java. It is an java application that is capable of doing lots of things apliccable to datamining. We will be using it to determine what is the best classification algorithm for our usecase. As we will be creating a java application of our own that can be used to classify the different bird groups. 

* First we use the explorer to make our datafiles usable in weka by removing duplicate id collums
* when the data is ready we can use the classify module in weka to test diffrent classification algorithm's that have reasonable accuracy's.
* at last when we found some interesting algorithm's we can use the experimenter to try diffrent settings to find optimal ones.

Every algorithm their initial output is a confusion matrix. This matrix is the measurement that we use to compare different algorithms. they are also inspected using a learning & ROC curve's.

* A Learning curve shows the false positive rate over how much data is provided to the learning algorithm
* A ROC curve shows the Sensitivity over the Specificity.

\newpage

    
  

## Resutls

To begin researching what classification algorithm is the best for our data we first need to explore the data. we want to know what the distribution of data is, if we need to remove datapoints because they are missing or misleading, so removing NA's & outliers. To find out what algorithms to use we can compare their performance by looking at accuracym ROC and their learning curve.


### Exploratory Data Analysis

The data contains 420 bird samples where the bone lengths and diameters have been measured. 
The birds are separated in 6 diffrent groups:

  * Swimming Birds, SW 
  * Wading Birds, W 
  * Terrestrial Birds, T 
  * Raptors, R 
  * Scansorial Birds, P 
  * Singing Birds, SO 

Most samples have data for:

  * Length and Diameter of the Humerus
  * Length and Diameter of the Ulna
  * Length and Diameter of the Femur
  * Length and Diameter of the Tibiotarsus
  * Length and Diameter of the Taesometatarsus

```{r echo=FALSE}
BirdBones <- read.csv("../data/bird.csv",header = T, sep = ",")
BirdBones.noNA <- BirdBones[complete.cases(BirdBones),]
length <- c(2,4,6,8,10)
# library(ggplot2)
# 
# ggplot(stack(BirdBones.noNA[length]), aes(x = ind, y = values, color = ind)) +
#   geom_boxplot()+
#   xlab("Bone Type")+
#   ylab("Length in mm")+
#   labs(title = "Boxplot for bone lenghts per bone in mm", 
#        subtitle = "Variation between small and large bones.",
#        caption = "Graph 1: Bone lengths")
```
<!-- Graph 1 shows the average lenght per bone for all ecological groups. As we can see there are 2 bone types which are quite a bit smaller than the other 3 bones. The Femuer(green) is the smallest of these 2, this stands out because in us humans this is our biggest bone. after the Femeur the next smallest bone is the Taesometatarsus, this is expected as this bone connects the feet of the birds to their legs. -->

<!-- These 2 bones are quite small and show less variation than the other 3 bones. As we are trying to classify these bones they might be less important in our final classification algorithm.  -->

```{r, echo=FALSE}
library(ggplot2)
library(grid)
library(gridExtra)
source("../scripts/BoneMeans.R")
length.long <- c(2, 4, 8)
diameter.long <- c(3, 5, 9)
BirdBones.noNA.long <- BoneMeans(BirdBones.noNA, length.long, diameter.long)

p1<- ggplot(BirdBones.noNA.long,aes(x=length.mean,y=diameter.mean,color=type))+
  geom_point()+
  geom_smooth(method = loess)+
  xlab("Mean Lenght in mm")+
  ylab("Mean Diameter in mm")+
  labs(title = "lenght vs mean diameter", caption = "graph 1: lenght vs diameter"
       )+
  theme(legend.position="bottom")

p2 <- ggplot(BirdBones.noNA.long,aes(x=log2(length.mean),y=log2(diameter.mean),color=type))+
  geom_point()+
  geom_smooth(method = loess)+
  xlab("Log2 Mean Lenght")+
  ylab("Log2 Mean Diameter")+
  labs(title = "Log2 transformed", caption = "graph 2: Log2 lenght vs diameter"
       )

g_legend<-function(a.gplot){
  tmp <- ggplot_gtable(ggplot_build(a.gplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)}

mylegend <-g_legend(p1)


p3 <- grid.arrange(arrangeGrob(p1 + theme(legend.position="none"),
                         p2 + theme(legend.position="none"),
                         nrow=1),
             mylegend, nrow=2,heights=c(10, 1))


```

Graph 1 & 2 show the bone length & diameter distribution. As expected the lenght and diameter are strongly correlated (longer bones alse need to be thicker to keep their strenght). 

Graph 1 is the distribution without any changes. Here we see that the Swimming birds have quite large bones, which could be because of abnormally big bird samples. as you look closer at the swimming birds in graph 1 and 2 we see that there might be a lot of noise in these samples because there are a lot of points outside the main distribution. For the other groups we also see that the Scansorial Birds & the Sininging birds are the smaller groups. 

Terrestrial Birds, Raptors and Wading Birds seem to be in the middle of the rest with the Terrestrial Birds having notisable thicker bones that the others.

### Data Cleaning
```{r, echo=FALSE}
library(ggplot2)
library(grid)
library(gridExtra)

 p1 <- ggplot(BirdBones.noNA.long, aes(x = type, y = length.mean, color = type)) +
  geom_boxplot()+
  ggtitle("Boxplot for each ecological group's mean bone lenght")+
  ylab("Mean bone lenght in mm")+
  xlab("ecological group")+
  labs("Boxplot for each ecological group's mean bone lenght", subtitle = "For Humerus, Ulna and Tibiotarsus",caption = "graph 3: mean bone lenghts")+
   theme(legend.position="none")





huml.3rd.q <- 90.31
huml.1st.q <-25.17

out <- huml.1st.q - 1.5*(huml.3rd.q - huml.1st.q)
out.large <- huml.3rd.q + 1.5*(huml.3rd.q - huml.1st.q)
outliers <- subset(BirdBones.noNA.long, huml > out.large | huml < out)
Birdbones.Clean <- BirdBones.noNA.long[! BirdBones.noNA.long$id %in% outliers$id, ]



p2 <- ggplot(Birdbones.Clean, aes(x = type, y = length.mean, color = type)) +
  geom_boxplot()+
  labs(
    title = "Boxplot for each ecological group's mean bone lenght",
    x = "ecological group",
    y = "",
    subtitle = "For Humerus, Ulna and Tibiotarsus",
    caption = "graph 4: Cleaned data"
  )+
  theme(legend.position="none")



grid.arrange(p1, p2, ncol = 2)
```
For our cleaning of data we removed all rows with NA's which cost us 7 datapoints. After that we calculated the 1st and 3rd quartile outliers from the humurus which can be seen in graph 3 & 4, and removed 9 rows.



* 1st quartile(Q1): 25.17 mm
* 3rd quartile(Q3): 90.31 mm

$Small threshold = Q1 - 1.5 * (Q3 - Q1)$

$Large threshold = Q3 + 1.5 * (Q3 - Q1)$

```{r}
library(pander)
NAs <- BirdBones[!complete.cases(BirdBones),]
huml.3rd.q <- 90.31
huml.1st.q <-25.17

out <- huml.1st.q - 1.5*(huml.3rd.q - huml.1st.q)
out.large <- huml.3rd.q + 1.5*(huml.3rd.q - huml.1st.q)
outliers <- subset(BirdBones, huml > out.large | huml < out)
summar <- as.table(c(nrow(NAs), nrow(outliers)))
row.names(summar) <- c("NA's", "Outliers")
pander(summar, caption = "Removed datapoints")
```


After all cleaning we left with 404 datapoints to use for Weka Analyses and classification.
\newpage

### Weka (classification)
In weka the goal was to find a classification algorithm that has the highest possible accuracy because classifying a birds herritage it does not matter that much if there is a False positive, the goal was to keep false negatives low and keep true positives high.



```{r, echo=FALSE}
accuracy <- rbind(c(342, 62 ), c("84.6535 %", "15.3465 %"))

colnames(accuracy) <- c("Correct", "Incorrect")
rownames(accuracy) <- c("Instances", "Percentage")
pander(accuracy, caption = "Random.Forest performance")

accuracy <- rbind(c(371, 33 ), c("91.8317 %", "8.1683 %"))

colnames(accuracy) <- c("Correct", "Incorrect")
rownames(accuracy) <- c("Instances", "Percentage")
pander(accuracy, caption = "Ibk performance")
```

```{r}
rocdataW <- read.csv("../data/RocWading.arff", header = F, comment.char = "@", na.strings = "?")
rocdataWibk <- read.csv("../data/RocWadingIbk.arff", header = F, comment.char = "@", na.strings = "?")

library(ggplot2)
ggplot()+
  geom_line(aes(x = rocdataW$V6, y = rocdataW$V7, color="Random.Forest"))+
  geom_line(aes(x = rocdataWibk$V6, y = rocdataWibk$V7, color="IBk"))+
  labs(x = "False Positive Rate(Specificity)",
       y = "True Positive Rate(Sensitivity)",
       title = "ROC curve",
       subtitle = "Based on algorithm preformance on Wading bird classification",
       caption = "Graph 5: ROC"
       )
```

In graph 5 we can see the ROC comparison between IBk and Random.Forest. As the two algorithm's come closer to the upper left we see that Ibk cuts off quit a bit earlier than Random.Forest. this could give an indication that Random.forest might have a higher overall accuracy. But in the IBk algorithm we got fewer datapoint to work with.

```{r}
RemovedPercentage <- c(10, 20, 30, 40, 50, 60, 70, 80, 90)
Random.Forest <- c(22.07, 23.27, 25.17, 26.82, 29.17, 32.59, 35.48, 39.24, 47.05)
IBk <- c(18.26, 19.08, 20.63, 23.01, 25.26, 28.52, 31.58, 35.75, 45.99)

dataf <- data.frame(rev(RemovedPercentage), Random.Forest, IBk)
dataf <- dataf[seq(dim(dataf)[1],1),]

library(ggplot2)
ggplot(dataf)+
  geom_smooth(aes(x = RemovedPercentage, y = Random.Forest, color="Random.Forest"), method = loess)+
  geom_smooth(aes(x = RemovedPercentage, y = IBk, color="IBk"), method = loess)+
  labs(x = "Percentage of data",
       y = "False Positive rate",
       title = "Learning curve",
       subtitle = "For Random.Forest && IBk",
       caption = "graph 6: Learning curve inspected algorithm's"
       )
  
```

Graph 6 shows the learning curve for both algorithms. As we reduce the amount of data fed to the learning algorithms we can see that both lose about the same amount of accuracy. But as shown in table 2 & 3 the overall accuracy of ibk is about 8% better and that is shown here again.

Ibk is also a small amount better in classification when less data is provided.



```{r}
humurus <- signif(84.6535 - 82.1782, 4)
ulna <-  signif(84.6535 -83.1683, 4) 
Femur <-  signif(84.6535 -80.4455, 4)
Tibiotarsus <-  signif(84.6535 - 84.4059, 4) 
Taesometatarsus <-  signif(84.6535-83.1683, 4)

loss <- data.frame(Bone=c("humurus", "ulna", "Femur", "Tibiotarsus", "Taesometatarsus"), 
                   Classify_loss=c(humurus, ulna, Femur, Tibiotarsus, Taesometatarsus))

library(ggplot2)
ggplot(data=loss, aes(x=Bone, y=Classify_loss)) +
  geom_bar(stat="identity", fill = "blue")+
  geom_text(aes(label=Classify_loss), vjust=1.6, color="white", size=3.5)+
  theme_minimal()+
  labs(x = "bone type",
       y = "accuracy loss (%)",
       title = "Classification loss if certain bones are removed",
       subtitle = "For Random.forest",
       caption = "graph 7: accuracy loss per bone"
       )
```
in graph 7 we can see what the importance is from certain bones if we are talking about classification. the differences we can see are because of their importance for the functionallity of the bird groups.

\newpage




## Conclusion & discussion

Are there bones in a bird that are more important for classification using machine learning? Or should we take all avalable information into account if we are creating a classificiaton algorithm using machine learning. And then what algorithm should we use? 

To answer part one of the question we can take a look at graph 7 here all different kinds of bones were listed and removed from classification one by one, and the results show that the Tibiotarsus collums in the data are the least important in classificaton. What is suprising is that the femeur which was the smallest from all inspected bones, has the highest classification accuracy loss. 

So if bones are missing in for example the fossil you're investigating the least important one is the Tibiotarsus but it is always preferred to have the most data availalbe.

For the algorithm to use we chose Random.Forest. Altough IBk gives better results the data is skewed and not all groups have the same amount of datapoints. which gave us reason to believe that the data was overfitted to the trainings data.

If we were to redo this classification experiment we would strongly recommend normalizing the data, because as said in our conclusion the data is skewed towards the Sininging and Swimming Birds. use a method that equalizes the amount of datapoints per ecological group so that there is no more data for one type than the oters.

Further we only removed outliers based on the Humerus. it might be worth looking at all available bones for results that differ too much. Also debate on removing datapoints with NA values as it might still be possible to classifiy if only one bone is missing as demonstrated in graph 7. If there is too much missing then yes, remove that instance but only one micht not be that important.

\newpage
## Minor Proposal

## References
